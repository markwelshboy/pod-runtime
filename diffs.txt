commit 1932027e9776b3608de6c172e65c5359e0efb248
Author: markwelshboy <mark.david.richards+git@gmail.com>
Date:   Tue Nov 25 17:48:49 2025 -0800

    WIP: 2025-11-25-17:48:49

diff --git a/helpers.sh b/helpers.sh
index c537ffc..295fa4f 100644
--- a/helpers.sh
+++ b/helpers.sh
@@ -31,7 +31,7 @@ shopt -s extglob
 # Optional (misc tooling):
 #   COMFY_REPO_URL       - ComfyUI repo URL (default comfyanonymous/ComfyUI)
 #   GIT_DEPTH            - default 1
-#   MAX_CUSTOM_NODE_JOBS        - default 6..8
+#   MAX_NODE_JOBS        - default 6..8
 # Hugging Face:
 #   HF_REPO_ID           - e.g. user/comfyui-bundles
 #   HF_REPO_TYPE         - dataset | model (default dataset)
@@ -45,7 +45,7 @@ shopt -s extglob
 # Provide reasonable fallbacks if .env forgot any
 : "${COMFY_REPO_URL:=https://github.com/comfyanonymous/ComfyUI}"
 : "${GIT_DEPTH:=1}"
-: "${MAX_CUSTOM_NODE_JOBS:=8}"
+: "${MAX_NODE_JOBS:=8}"
 : "${HF_API_BASE:=https://huggingface.co}"
 : "${CN_BRANCH:=main}"
 : "${CACHE_DIR:=${COMFY_HOME:-/tmp}/cache}"
@@ -102,7 +102,7 @@ ensure_base_deps() {
 #  Path + directory helpers  #
 # -------------------------- #
 
-ensure_dirs(){
+ensure_comfy_dirs() {
   mkdir -p \
     "${COMFY_HOME:?}" \
     "${CUSTOM_DIR:?}" \
@@ -121,76 +121,37 @@ ensure_dirs(){
     "${VAE_DIR:?}" \
     "${LORAS_DIR:?}" \
     "${DETECTION_DIR:?}" \
-    "${CTRLNET_DIR:?}" \
-    "${SAMS_DIR:?}" \
-    "${UPSCALE_DIR:?}" \
-    "${CTRLNET_DIR}/SDXL/controlnet-union-sdxl-1.0"
+    "${CTRL_DIR:?}" \
+    "${UPSCALE_DIR:?}"
+
+}
+
+ensure_workspace() {
+  if [ ! -d /workspace ]; then
+    mkdir -p /workspace
+  fi
 }
 
 # ------------------------- #
-#  Workflows / Asset import #
+#  Workflows / Icons import #
 # ------------------------- #
-copy_hearmeman_files_from_repo_if_any() {
+copy_hearmeman_assets_if_any(){
   local repo="${HEARMEMAN_REPO:-}"
-  if [[ -z "$repo" ]]; then
-    return 0
-  fi
-
+  if [ -z "$repo" ]; then return 0; fi
   local tmp="${CACHE_DIR}/.hearmeman.$$"
   rm -rf "$tmp"
-
-  echo "[hearmeman] Syncing assets from: ${repo}"
-  echo "[hearmeman] Temp repo location:  ${tmp}"
-
-  if ! git clone "$repo" "$tmp" >/dev/null 2>&1; then
-    echo "[hearmeman] âŒ Failed to clone repo; skipping repo sync." >&2
-    rm -rf "$tmp"
-    return 0
-  fi
-
-  # ---- Workflows ----
-  local wf_src=""
-  if [[ -d "$tmp/src/workflows" ]]; then
-    wf_src="$tmp/src/workflows"
-  elif [[ -d "$tmp/workflows" ]]; then
-    wf_src="$tmp/workflows"
-  fi
-
-  if [[ -n "$wf_src" ]]; then
-    echo "[hearmeman] Relocating workflows found in repo."
-    local wf_dst="${COMFY_HOME}/workflows"
-    echo "[hearmeman] Workflows source:    ${wf_src}"
-    echo "[hearmeman] Workflows dest:      ${wf_dst}"
-    mkdir -p "$wf_dst"
-    cp -rf "${wf_src}/"* "$wf_dst"/ 2>/dev/null || true
-  else
-    echo "[hearmeman] No workflows found under $repo/src/workflows or $repo/workflows." >&2
+  git clone "$repo" "$tmp" || return 0
+  # Workflows
+  if [ -d "$tmp/src/workflows" ]; then
+    mkdir -p "${COMFY_HOME}/workflows"
+    cp -rf "$tmp/src/workflows/"* "${COMFY_HOME}/workflows/" || true
   fi
-  
-  echo ""
-
-  # ---- Assets ----
-  local assets_src=""
-  if [[ -d "$tmp/src/assets" ]]; then
-    assets_src="$tmp/src/assets"
-  elif [[ -d "$tmp/assets" ]]; then
-    assets_src="$tmp/assets"
-  fi
-
-  if [[ -n "$assets_src" ]]; then
-    echo "[hearmeman] Relocating assets found in repo."
-    local assets_dst="${COMFY_HOME}/assets"
-    echo "[hearmeman] Assets source:       ${assets_src}"
-    echo "[hearmeman] Assets dest:         ${assets_dst}"
-    mkdir -p "$assets_dst"
-    cp -rf "${assets_src}/"* "$assets_dst"/ 2>/dev/null || true
-  else
-    echo "[hearmeman] No assets found under $repo/src/assets or $repo/assets." >&2
+  # Icons / scripts (e.g., start.sh images)
+  if [ -d "$tmp/src/assets" ]; then
+    mkdir -p "${COMFY_HOME}/assets"
+    cp -rf "$tmp/src/assets/"* "${COMFY_HOME}/assets/" || true
   fi
-
   rm -rf "$tmp"
-  echo ""
-  echo "[hearmeman] Repo sync complete."
 }
 
 # ======================================================================
@@ -553,133 +514,13 @@ rewrite_custom_nodes_requirements() {
   done
 }
 
-# -------------------------------------------------------------------
-# Snapshot custom_nodes state: name, git SHA, branch, remote URL
-# Writes to ${COMFY_LOGS}/custom_nodes.snapshot
-# -------------------------------------------------------------------
-snapshot_custom_nodes_state() {
-  local mode="full"
-  local stage=""
-
-  # ----------------------------
-  # Parse arguments:
-  #   snapshot_custom_nodes_state [--summary] [--stage STAGE] [STAGE]
-  # ----------------------------
-  while [[ $# -gt 0 ]]; do
-    case "$1" in
-      --summary)
-        mode="summary"
-        shift
-        ;;
-      --stage)
-        stage="${2:-}"
-        shift 2
-        ;;
-      --stage=*)
-        stage="${1#--stage=}"
-        shift
-        ;;
-      *)
-        # Bare positional becomes the stage/tag if not already set
-        if [[ -z "$stage" ]]; then
-          stage="$1"
-        fi
-        shift
-        ;;
-    esac
-  done
-
-  local root="${CUSTOM_DIR:-${COMFY_HOME:-/workspace/ComfyUI}/custom_nodes}"
-  local logs_root="${COMFY_LOGS:-/workspace/logs}"
-  local out="${logs_root}/custom_nodes.snapshot"
-
-  mkdir -p "$logs_root"
-
-  # Small helper to format the header line
-  local now; now="$(date -Is)"
-  local stage_suffix=""
-  if [[ -n "$stage" ]]; then
-    stage_suffix=" [stage: ${stage}]"
-  fi
-
-  if [[ ! -d "$root" ]]; then
-    local msg="[custom-nodes] Snapshot${stage_suffix}: no custom_nodes dir at ${root}"
-    if [[ "$mode" == "summary" ]]; then
-      echo "$msg" | tee -a "$out"
-    else
-      echo "$msg" | tee -a "$out"
-    fi
-    return 0
-  fi
-
-  # Collect entries once so we can count and iterate
-  local -a dirs=()
-  while IFS= read -r d; do
-    dirs+=("$d")
-  done < <(find "$root" -maxdepth 1 -mindepth 1 -type d 2>/dev/null | sort)
-
-  local count="${#dirs[@]}"
-
-  if [[ "$mode" == "summary" ]]; then
-    # ----------------------------
-    # Summary mode:
-    #   - One-line summary to screen
-    #   - Full list to the snapshot file only
-    # ----------------------------
-    {
-      echo "============================================================"
-      echo "  Custom Nodes Snapshot @ ${now}${stage_suffix}"
-      echo "  Root: ${root}"
-      echo "  Total nodes: ${count}"
-      echo "============================================================"
-      for d in "${dirs[@]}"; do
-        printf '%s\n' "$(basename "$d")"
-      done
-      echo
-    } >> "$out"
-
-    echo "[custom-nodes] Snapshot${stage_suffix}: ${count} node(s); details: ${out}"
-  else
-    # ----------------------------
-    # Full mode:
-    #   - Detailed git info to BOTH screen and file (via tee)
-    # ----------------------------
-    {
-      echo "============================================================"
-      echo "  Custom Nodes Snapshot @ ${now}${stage_suffix}"
-      echo "  Root: ${root}"
-      echo "============================================================"
-      if [[ "$count" -eq 0 ]]; then
-        echo "(no subdirectories under custom_nodes)"
-      else
-        printf '%-32s  %-10s  %-10s  %s\n' "Custom Node (Name/Dir)" "SHA" "Branch" "GIT Remote"
-        printf '%-32s  %-10s  %-10s  %s\n' "======================" "==========" "==========" "======================"
-        for d in "${dirs[@]}"; do
-          local name sha branch remote
-          name="$(basename "$d")"
-
-          if [[ -d "$d/.git" ]] && command -v git >/dev/null 2>&1; then
-            sha="$(git -C "$d" rev-parse --short HEAD 2>/dev/null || echo "unknown")"
-            branch="$(git -C "$d" rev-parse --abbrev-ref HEAD 2>/dev/null || echo "unknown")"
-            remote="$(git -C "$d" config --get remote.origin.url 2>/dev/null || echo "unknown")"
-            printf '%-32s  %-10s  %-10s  [%s]\n' "$name" "$sha" "$branch" "$remote"
-          else
-            printf '%-32s  %s\n' "$name" "<not a git repo>"
-          fi
-        done
-      fi
-      echo
-    } | tee -a "$out"
-  fi
-}
-
 # install_custom_nodes:
 #   - Optional arg: manifest source (file path or URL)
 #   - If no arg, falls back to CUSTOM_NODES_MANIFEST_URL
 #   - Manifest format (plain text, one entry per line):
 #       <git_url> <target_dir> [optional git clone args...]
 #     Lines starting with # or empty are ignored.
-#   - Uses clone_or_pull/build_node as the core, in parallel up to MAX_CUSTOM_NODE_JOBS.
+#   - Uses clone_or_pull/build_node as the core, in parallel up to MAX_NODE_JOBS.
 install_custom_nodes() {
   _helpers_need curl
 
@@ -707,14 +548,12 @@ install_custom_nodes() {
   local log_dir="${CUSTOM_LOG_DIR:-${COMFY_LOGS:-/workspace/logs}/custom_nodes}"
   mkdir -p "$custom_dir" "$log_dir"
 
-  local max_jobs="${MAX_CUSTOM_NODE_JOBS:-8}"
+  local max_jobs="${MAX_NODE_JOBS:-8}"
   local job_count=0
   local failed=0
 
   echo "[custom-nodes] Using manifest: $src"
   echo "[custom-nodes] Installing custom nodes into: $custom_dir"
-  echo "[custom-nodes] Running maximum of ${MAX_CUSTOM_NODE_JOBS} parallel install jobs. Configure with MAX_CUSTOM_NODE_JOBS."
-  echo
   cd "$custom_dir"
 
   local line url dst rest
@@ -781,7 +620,6 @@ install_custom_nodes() {
   fi
 
   echo "[custom-nodes] Manifest install completed successfully."
-
   return 0
 }
 
@@ -801,7 +639,7 @@ install_custom_nodes_set() {
   mkdir -p "${CUSTOM_DIR:?}" "${CUSTOM_LOG_DIR:?}"
 
   # Concurrency
-  local max_jobs="${MAX_CUSTOM_NODE_JOBS:-8}"
+  local max_jobs="${MAX_NODE_JOBS:-8}"
   if ! [[ "$max_jobs" =~ ^[0-9]+$ ]] || (( max_jobs < 1 )); then max_jobs=8; fi
   echo "[custom-nodes] Using concurrency: ${max_jobs}"
 
@@ -867,19 +705,17 @@ install_custom_nodes_set() {
 
 hf_ensure_local_repo() {
   local repo="${HF_LOCAL_REPO:-${CACHE_DIR}/.hf_repo}"
-  local url display_url
+  local url
 
   url="$(hf_remote_url 2>/dev/null || true)"
   if [[ -z "$url" ]]; then
     echo "[hf-ensure-local-repo] hf_ensure_local_repo: hf_remote_url unresolved" >&2
     return 1
+  else
+    local display_url="${$url/https:\/\/oauth2:[^@]*@/https:\/\/oauth2:***@}"
+    echo "[hf-ensure-local-repo] Master repo url=${display_url}" >&2
   fi
 
-  # Redact token just for logging:
-  # transforms: https://oauth2:<TOKEN>@huggingface.co/...  ->  https://oauth2:***@huggingface.co/...
-  display_url="$(printf '%s\n' "$url" | sed -E 's#(https://oauth2:)[^@]*@#\1***@#')"
-  echo "[hf-ensure-local-repo] Master repo url=${display_url}" >&2
-
   echo "[hf-ensure-local-repo] Using local repo=${repo}" >&2
 
   if [[ -d "$repo/.git" ]]; then
@@ -890,13 +726,13 @@ hf_ensure_local_repo() {
   else
     echo "[hf-ensure-local-repo] Cloning HF repo into $repoâ€¦" >&2
     mkdir -p "$(dirname "$repo")"
-    if ! git clone --depth=1 "$url" "$repo" >/dev/null 2>&1; then
+    git clone --depth=1 "$url" "$repo" >/dev/null 2>&1 || {
       echo "[hf-ensure-local-repo] âŒ Failed to clone HF repo into $repo" >&2
       return 1
-    fi
+    }
   fi
 
-  printf '%s\n' "$repo"
+  echo "$repo"
 }
 
 # hf_remote_url: builds authenticated HTTPS remote for model/dataset repos
@@ -1150,7 +986,7 @@ hf_repo_info() {
   type="${HF_REPO_TYPE:-dataset}"
   url="https://huggingface.co/${type}s/${ns}/${name}"
 
-  echo "========================================================================"
+  echo "=================================================="
   echo "ðŸ¤– Hugging Face Repo Info"
   echo "Repo handle    : ${ns}/${name}"
   echo "Repo type      : ${type}"
@@ -1178,7 +1014,7 @@ hf_repo_info() {
     echo "Torch key      : ${key}"
   fi
 
-  echo "========================================================================"
+  echo "=================================================="
 }
 
 # Return 0 if a LOCAL file exists for given basename in $CACHE_DIR
@@ -1704,12 +1540,12 @@ PY
   echo "[sage-bundle] [install_sage_from_source] TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}" >&2
 
   # Clean + build (NO build isolation, so setup can import torch)
-  echo "[sage-bundle] [install_sage_from_source] Cloning SageAttention, repo=https://github.com/thu-ml/SageAttention.git" >&2
+  echo "[sage-bundle] [install_sage_from_source] Clonging SageAttention, repo=https://github.com/thu-ml/SageAttention.git" >&2
   rm -rf /tmp/SageAttention
   git clone https://github.com/thu-ml/SageAttention.git /tmp/SageAttention
 
   if env -u PIP_REQUIRE_HASHES -u PIP_BUILD_CONSTRAINT $PIP install --no-build-isolation -e /tmp/SageAttention 2>&1 | tee /workspace/logs/sage_build.log; then
-    echo "[sage-bundle] [install_sage_from_source] âœ… SageAttention built OK"
+    echo "[sage-bundle] [install_sage_from_source] SageAttention built OK"
     return 0
   else
     echo "[sage-bundle] [install_sage_from_source] SageAttention build FAILED â€” see /workspace/logs/sage_build.log" >&2
@@ -1849,13 +1685,13 @@ hf_fetch_sage_bundle() {
   echo "[sage-bundle] [hf_fetch_sage_bundle] Looking for matching bundle $(basename $patt)." >&2
 
   if [[ ! -f "$patt" ]]; then
-    echo "[sage-bundle] [hf_fetch_sage_bundle] Exact bundle not found for key=${key}." >&2
+    echo "[sage-bundle] [hf_fetch_sage_bundle] âŒ Exact bundle not found for key=${key}." >&2
     return 1
   fi
 
   mkdir -p "$CACHE_DIR"
   local_tgz="${CACHE_DIR}/$(basename "$patt")"
-  echo "[sage-bundle] [hf_fetch_sage_bundle] âœ… Found SageAttention bundle. Copying â†’ ${local_tgz}." >&2
+  echo "[sage-bundle] [hf_fetch_sage_bundle] Found bundle. Copying â†’ ${local_tgz}." >&2
   cp -f "$patt" "$local_tgz"
 
   echo "$local_tgz"
@@ -1896,7 +1732,7 @@ hf_fetch_pip_cache() {
 
   local local_tgz="${CACHE_DIR}/pip_cache_${key}.tgz"
   mkdir -p "${CACHE_DIR:-/workspace/ComfyUI/cache}"
-  echo "[pip-cache] [hf_fetch_pip_cache] âœ… Found bundle. Copying â†’ ${local_tgz}." >&2
+  echo "[pip-cache] [hf_fetch_pip_cache] Found bundle. Copying â†’ ${local_tgz}." >&2
   cp -f "$patt" "$local_tgz"
 
   echo "$local_tgz"
@@ -1931,7 +1767,7 @@ push_pip_cache_if_requested() {
   }
 
   hf_push_files "pip_cache ${key}" "$tgz"
-  echo "[pip-cache] [push_pip_cache_if_requested] âœ… Uploaded pip_cache_${key}.tgz" >&2
+  echo "[pip-cache] [push_pip_cache_if_requested] Uploaded pip_cache_${key}.tgz" >&2
 }
 
 # ensure_pip_cache_for_custom_nodes:
@@ -1961,7 +1797,7 @@ ensure_pip_cache_for_custom_nodes() {
       tar -xzf "$tgz" -C "$cache" || {
         echo "[pip-cache] [ensure_pip_cache_for_custom_nodes] âš ï¸ Failed to extract pip cache archive; continuing without it." >&2
       }
-      echo "[pip-cache] [ensure_pip_cache_for_custom_nodes] âœ… Restored pip cache from tar file." >&2
+      echo "[pip-cache] [ensure_pip_cache_for_custom_nodes] Restored pip cache from tar file." >&2
     fi
   fi
 }
@@ -1992,7 +1828,7 @@ import sys
 print("[sage-bundle] [restore_sage_from_tar] sys.executable:", sys.executable)
 try:
     import sageattention
-    print("[sage-bundle] [restore_sage_from_tar] âœ… SAGE imported sucessfully from tar bundle:", sageattention, getattr(sageattention, "__file__", None))
+    print("[sage-bundle] [restore_sage_from_tar] SAGE imported sucessfully from tar bundle:", sageattention, getattr(sageattention, "__file__", None))
 except Exception as e:
     print("[sage-bundle] [restore_sage_from_tar] SAGE IMPORT ERROR:", repr(e))
 PY
@@ -2055,7 +1891,7 @@ hf_fetch_latest_custom_nodes_bundle() {
   local repo patt best
 
   repo="$(hf_ensure_local_repo)" || {
-    echo "[custom-nodes] Could not initialize local repo" >&2
+    echo "[custom-nodes] hf_fetch_latest_custom_nodes_bundle: no local repo" >&2
     return 1
   }
 
@@ -2080,7 +1916,7 @@ hf_fetch_custom_nodes_requirements_for_tag() {
   local repo req_src req_dst
 
   repo="$(hf_ensure_local_repo)" || {
-    echo "[custom-nodes] hf_fetch_latest_custom_nodes_bundle: Could not initialize local repo" >&2
+    echo "[custom-nodes] hf_fetch_latest_custom_nodes_bundle: no local repo" >&2
     return 1
   }
 
@@ -2106,7 +1942,7 @@ install_custom_nodes_requirements() {
 
   local repo req_src req_work
   repo="$(hf_ensure_local_repo)" || {
-    echo "[custom-nodes] hf_fetch_latest_custom_nodes_bundle: Could not initialize local repo" >&2
+    echo "[custom-nodes] hf_fetch_latest_custom_nodes_bundle: no local repo" >&2
     return 1
   }
 
@@ -2262,7 +2098,7 @@ build_custom_nodes_bundle() {
   fi
   tar -C "$(dirname "$CUSTOM_DIR")" -czf "$tarpath" "$(basename "$CUSTOM_DIR")"
   sha256sum "$tarpath" > "$sha"
-  echo "[custom-nodes] [build_custom_nodes_bundle] âœ… Built manifest, reqs, tar, sha256 bundle for HF upload." >&2
+  echo "[custom-nodes] [build_custom_nodes_bundle] Built manifest, reqs, tar, sha256 bundle for HF upload." >&2
   echo "$tarpath"
 }
 
@@ -2307,7 +2143,7 @@ ensure_custom_nodes_from_bundle_or_build() {
       rm -rf "$CUSTOM_DIR"
       mkdir -p "$(dirname "$CUSTOM_DIR")"
       tar -xzf "$tgz" -C "$(dirname "$CUSTOM_DIR")"
-      echo "[custom-nodes] [ensure_custom_nodes_from_bundle_or_build] âœ… Restored custom nodes from HF bundle."
+      echo "[custom-nodes] [ensure_custom_nodes_from_bundle_or_build] Restored custom nodes from HF bundle."
 
       if install_custom_nodes_requirements "$tag"; then
         push_bundle_if_requested || true
@@ -2594,14 +2430,9 @@ aria2_show_download_snapshot() {
   pending_count="$(jq -r 'length' <<<"$wai" 2>/dev/null || echo 0)"
   completed_count="$(jq -r 'length' <<<"$sto" 2>/dev/null || echo 0)"
 
-  echo
   echo "================================================================================"
   echo "=== Aria2 Downloader Snapshot @ $(date '+%Y-%m-%d %H:%M:%S')"
-  echo "================================================================================"
-  echo "==="
   echo "=== Active: $active_count   Pending: $pending_count   Completed: $completed_count"
-  echo "==="
-  echo "=== Max Concurrent Downloads (ARIA2_MAX_CONC)=${ARIA2_MAX_CONC}"
   echo "================================================================================"
   echo
 
@@ -2690,7 +2521,7 @@ aria2_show_download_snapshot() {
         dir="${dir#$ROOT/}"
       fi
 
-      printf " %3d%% [%-*s] %8s/s (%6s / %6s)  [ %-23s ] <- %s\n" \
+      printf " %3d%% [%-*s] %8s/s (%6s / %6s)  [ %-28s ] %s\n" \
         "$pct" "$W" "$bar" \
         "$(helpers_human_bytes "$spd")" \
         "$(helpers_human_bytes "$done")" \
@@ -3100,6 +2931,9 @@ aria2_enqueue_and_wait_from_manifest() {
     "${ARIA2_PROGRESS_BAR_WIDTH:-40}" \
     "${COMFY_LOGS:-/workspace/logs}/aria2_progress.log"
 
+  # Show completed block when done
+  helpers_print_completed_from_aria2 2>/dev/null || true
+
   aria2_clear_results >/dev/null 2>&1 || true
   trap - INT TERM
   return 0
@@ -3515,185 +3349,41 @@ aria2_enqueue_and_wait_from_civitai() {
   return 0
 }
 
-# Returns a human-readable HF token status on stdout
-hf_token_status() {
-  # Safe with `set -u`
-  local token="${HF_TOKEN:-}"
-
-  # 1) Not set at all
-  if [[ -z "$token" ]]; then
-    printf '%s' "âŒ not set"
-    return 1
-  fi
-
-  # 2) GUI default / placeholder
-  if [[ "$token" == "token_here" || "$token" == "HF_TOKEN_HERE" ]]; then
-    printf '%s' "âš ï¸ placeholder"
-    return 1
-  fi
-
-  # 3) Can't validate if curl is missing
-  if ! command -v curl >/dev/null 2>&1; then
-    printf '%s' "âš ï¸ set (curl missing, can't validate)"
-    return 0
-  fi
-
-  # 4) Call HF whoami-v2 to validate
-  local code
-  code="$(curl -fsS --max-time 5 \
-           -o /dev/null -w '%{http_code}' \
-           -H "Authorization: Bearer $token" \
-           "https://huggingface.co/api/whoami-v2" 2>/dev/null || echo "000")"
-
-  case "$code" in
-    200)
-      printf '%s' "âœ… HF_TOKEN valid (whoami-v2 OK)"
-      return 0
-      ;;
-    401|403)
-      printf '%s' "âŒ HF_TOKEN invalid / unauthorized"
-      return 1
-      ;;
-    000)
-      printf '%s' "âš ï¸ HF_TOKEN set (network error talking to HF)"
-      return 1
-      ;;
-    *)
-      printf '%s' "âš ï¸ HF_TOKEN set (whoami-v2 HTTP $code)"
-      return 1
-      ;;
-  esac
-}
-
-civitai_token_status() {
-  local token="${CIVITAI_TOKEN:-}"
-
-  # 1) Not set
-  if [[ -z "$token" ]]; then
-    printf '%s' "âŒ not set"
-    return 1
-  fi
-
-  # 2) Common placeholder patterns
-  if [[ "$token" == "token_here" || "$token" == "CIVITAI_TOKEN_HERE" ]]; then
-    printf '%s' "âš ï¸ placeholder value"
-    return 1
-  fi
-
-  # 3) Ensure curl exists
-  if ! command -v curl >/dev/null 2>&1; then
-    printf '%s' "âš ï¸ set (curl missing, can't validate)"
-    return 0
-  fi
-
-  # 4) Validate against Civitai
-  local code
-  code="$(curl -fsS --max-time 5 \
-           -o /dev/null -w '%{http_code}' \
-           -H "Authorization: Bearer $token" \
-           "https://civitai.com/api/v1/auth/me" \
-           2>/dev/null || echo "000")"
-
-  case "$code" in
-    200)
-      printf '%s' "âœ… CIVITAI_TOKEN valid (auth/me OK)"
-      return 0
-      ;;
-    401)
-      printf '%s' "âŒ CIVITAI_TOKEN invalid (401)"
-      return 1
-      ;;
-    403)
-      printf '%s' "âŒ CIVITAI_TOKEN forbidden (403 â€” wrong scopes?)"
-      return 1
-      ;;
-    000)
-      printf '%s' "âš ï¸ CIVITAI_TOKEN set (network error)"
-      return 1
-      ;;
-    *)
-      printf '%s' "âš ï¸ CIVITAI_TOKEN generated unexpected HTTP $code"
-      return 1
-      ;;
-  esac
-}
-
-show_gpu () {
-  # --- Gather info safely ---
-  local gpustr arch
-  local gpu_cc gpu_label
-
-  gpustr="$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -n1 || echo "no-gpu")"
-  gpu_cc="$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader 2>/dev/null | head -n1 || echo "?")"
-  arch="${gpu_cc//./}"   # "12.0" â†’ "120"
-
-  # --- ASCII GPU header ---
-  gpu_label="GPU: ${gpustr}"
-  if [[ -n "$arch" && "$arch" != "?" ]]; then
-    gpu_label+="  (sm_${arch})"
-  fi
-  local width=${#gpu_label}
-  local border
-  border="$(printf 'â”€%.0s' $(seq 1 "$width"))"
-
-  echo "â”Œâ”€${border}â”€â”"
-  printf "â”‚ %-${width}s â”‚\n" "$gpu_label"
-  echo "â””â”€${border}â”€â”˜"
-}
-
-show_download_environment_variables () {
-  
-  compgen -A variable | grep "download_" | while IFS= read -r var_name; do
-    printf " %-39s%s\n" "${var_name}:" "${!var_name}"
-  done
-}
-
 show_env () {
-
   # ----- Convenience environment echo -----
-  echo "========================================================================"
-  echo ""
-
-  show_gpu
-
+  echo "======================================="
+  echo "ðŸ§  Environment Summary"
+  echo "======================================="
   echo ""
-  echo "========================================================================"
-  echo "ðŸ§  Environment Summary â€” $(date -Is)"
-  echo "========================================================================"
+  echo "COMFY_HOME:           $COMFY_HOME"
   echo ""
-  echo " COMFY_HOME:                            $COMFY_HOME"
-  echo " Comfy version:                         $(detect_comfy_version || echo unknown)"
+  echo "Custom nodes dir:     $CUSTOM_DIR"
+  echo "Cache dir:            $CACHE_DIR"
+  echo "Logs dir:             $COMFY_LOGS"
+  echo "Output dir:           $OUTPUT_DIR"
+  echo "Bundles dir:          $BUNDLES_DIR"
+  echo "Bundle tag:           $CUSTOM_NODES_BUNDLE_TAG"
+  echo "Workflow dir:         $WORKFLOW_DIR"
+  echo "Model manifest URL:   $MODEL_MANIFEST_URL"
   echo ""
-  echo " Custom nodes dir:                      $CUSTOM_DIR"
-  echo " Cache dir:                             $CACHE_DIR"
-  echo " Logs dir:                              $COMFY_LOGS"
-  echo " Output dir:                            $OUTPUT_DIR"
-  echo " Bundles dir:                           $BUNDLES_DIR"
-  echo " Workflow dir:                          $COMFY_WORKFLOW_DIR"
-  echo ""
-  echo " Custom Node Manifest:                  $CUSTOM_NODES_MANIFEST_URL"
-  echo " Model manifest:                        $MODEL_MANIFEST_URL"
-  echo ""
-
-  show_download_environment_variables
-
+  echo "DIFFUSION_MODELS_DIR: $DIFFUSION_MODELS_DIR"
+  echo "TEXT_ENCODERS_DIR:    $TEXT_ENCODERS_DIR"
+  echo "CLIP_VISION_DIR:      $CLIP_VISION_DIR"
+  echo "VAE_DIR:              $VAE_DIR"
+  echo "LORAS_DIR:            $LORAS_DIR"
+  echo "DETECTION_DIR:        $DETECTION_DIR"
+  echo "CTRL_DIR:             $CTRL_DIR"
+  echo "UPSCALE_DIR:          $UPSCALE_DIR"
   echo ""
-  echo " DIFFUSION_MODELS_DIR:                  $DIFFUSION_MODELS_DIR"
-  echo " TEXT_ENCODERS_DIR:                     $TEXT_ENCODERS_DIR"
-  echo " CLIP_VISION_DIR:                       $CLIP_VISION_DIR"
-  echo " VAE_DIR:                               $VAE_DIR"
-  echo " LORAS_DIR:                             $LORAS_DIR"
-  echo " DETECTION_DIR:                         $DETECTION_DIR"
-  echo " CTRLNET_DIR:                           $CTRLNET_DIR"
-  echo " UPSCALE_DIR:                           $UPSCALE_DIR"
-  echo " SAMS_DIR:                              $SAMS_DIR"
+  echo "HF_TOKEN:             $(if [ -n "$HF_TOKEN" ]; then echo "Set"; else echo "Not set"; fi)"
+  echo "CIVITAI_TOKEN:        $(if [ -n "$CIVITAI_TOKEN" ]; then echo "Set"; else echo "Not set"; fi)"
+  echo "CHECKPOINT_IDS:       ${CHECKPOINT_IDS_TO_DOWNLOAD:-Empty}"
+  echo "LORAS_IDS:            ${LORAS_IDS_TO_DOWNLOAD:-Empty}"
+  echo "======================================="
   echo ""
-  echo " HF_TOKEN:                              $(hf_token_status)"
-  echo " CIVITAI_TOKEN:                         $(civitai_token_status)"
-  echo " CHECKPOINT_IDS:                        ${CHECKPOINT_IDS_TO_DOWNLOAD:-Empty}"
-  echo " LORAS_IDS:                             ${LORAS_IDS_TO_DOWNLOAD:-Empty}"
+  hf_repo_info
   echo ""
-  echo "========================================================================"
+  echo "======================================="
 
 }
 
@@ -3730,40 +3420,11 @@ PY
   echo "$ver"
 }
 
-detect_comfy_version() {
-  local dir="${COMFY_HOME:-/workspace/ComfyUI}"
-  local ver="unknown"
-
-  if [[ -d "$dir/.git" ]] && command -v git >/dev/null 2>&1; then
-    local desc
-    desc="$(git -C "$dir" describe --tags --dirty --always 2>/dev/null || true)"
-
-    # Match vX.Y.Z-N-gSHA
-    if [[ "$desc" =~ ^v([0-9]+\.[0-9]+\.[0-9]+)-([0-9]+)-g[0-9a-f]+ ]]; then
-      local base="${BASH_REMATCH[1]}"
-      local extra="${BASH_REMATCH[2]}"
-      if [[ "$extra" -eq 0 ]]; then
-        ver="v${base}"
-      else
-        ver="v${base}+${extra}"
-      fi
-    # Pure tag (vX.Y.Z)
-    elif [[ "$desc" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
-      ver="$desc"
-    # Fallback: just use whatever describe gave us
-    elif [[ -n "$desc" ]]; then
-      ver="$desc"
-    fi
-  fi
-
-  echo "$ver"
-}
-
 # --------------------------------------------------
 # Pretty boot banner for Vast / RunPod logs
 # --------------------------------------------------
 on_start_training_banner() {
-  local logfile="${TRAINING_LOGS_DIR:-/workspace/training_logs}/startup_banner.log"
+  local logfile="${COMFY_LOGS:-/workspace/logs}/startup_banner.log"
   mkdir -p "$(dirname "$logfile")"
 
   # --- Gather info safely ---
@@ -3778,6 +3439,8 @@ on_start_training_banner() {
   gpu_cc="$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader 2>/dev/null | head -n1 || echo "?")"
   arch="${gpu_cc//./}"   # "12.0" â†’ "120"
 
+  comfyver=$(probe_comfy_version)
+
   # --- ASCII GPU header ---
   gpu_label="GPU: ${gpustr}"
   if [[ -n "$arch" && "$arch" != "?" ]]; then
@@ -3794,7 +3457,7 @@ on_start_training_banner() {
     echo "â””â”€${border}â”€â”˜"
     echo ""
     echo "============================================================"
-    echo "   ðŸš€ TRAINING BOOTSTRAP START (Vast) â€” $(date -Is)"
+    echo "   ðŸš€ COMFYUI BOOTSTRAP START (Vast) â€” $(date -Is)"
     echo "============================================================"
     echo " Image Tag:          ${IMAGE_TAG:-unknown}"
     echo " Build Git SHA:      ${BUILD_GIT_SHA:-unknown}"
@@ -3806,11 +3469,19 @@ on_start_training_banner() {
     echo " Torch:              ${torchver}"
     echo " CUDA (Torch):       ${cudaver}"
     echo ""
+    echo " ComfyUI Path:       ${COMFY_HOME:-/workspace/ComfyUI}"
+    echo " ComfyUI Version:    ${comfyver}"
+    echo ""
     echo " Model Manifest:     ${MODEL_MANIFEST_URL:-unset}"
+    echo " Node Manifest:      ${CUSTOM_NODES_MANIFEST_URL:-unset}"
     echo ""
-    echo " ENABLE_SAGE       = ${ENABLE_SAGE:-true}"
+    echo " ENABLE_MODEL_MANIFEST_DOWNLOAD = ${ENABLE_MODEL_MANIFEST_DOWNLOAD:-1}"
+    echo " ENABLE_CIVITAI_DOWNLOAD        = ${ENABLE_CIVITAI_DOWNLOAD:-1}"
+    echo " ENABLE_SAGE                    = ${ENABLE_SAGE:-1}"
+    echo " INSTALL_EXTRA_CUSTOM_NODES     = ${INSTALL_EXTRA_CUSTOM_NODES:-1}"
+    echo " LAUNCH_JUPYTER                 = ${LAUNCH_JUPYTER:-0}"
     echo ""
-    echo " Log Directory:      ${TRAINING_LOGS_DIR:-/workspace/training_logs}"
+    echo " Log Directory:      ${COMFY_LOGS:-/workspace/logs}"
     echo "============================================================"
     echo ""
   } | tee "$logfile"
@@ -3824,45 +3495,60 @@ on_start_comfy_banner() {
   mkdir -p "$(dirname "$logfile")"
 
   # --- Gather info safely ---
-  local pyver torchver cudaver comfyver
+  local pyver torchver cudaver gpustr arch comfyver
+  local gpu_cc gpu_label
 
   pyver="$($PY -c 'import sys; print(".".join(map(str, sys.version_info[:3])))' 2>/dev/null || echo "?")"
   torchver="$($PY -c 'import torch; print(torch.__version__)' 2>/dev/null || echo "not-importable")"
   cudaver="$($PY -c 'import torch; print(torch.version.cuda)' 2>/dev/null || echo "?")"
 
-  {
-    echo ""
+  gpustr="$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -n1 || echo "no-gpu")"
+  gpu_cc="$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader 2>/dev/null | head -n1 || echo "?")"
+  arch="${gpu_cc//./}"   # "12.0" â†’ "120"
 
-    show_gpu
+  comfyver=$(probe_comfy_version)
 
+  # --- ASCII GPU header ---
+  gpu_label="GPU: ${gpustr}"
+  if [[ -n "$arch" && "$arch" != "?" ]]; then
+    gpu_label+="  (sm_${arch})"
+  fi
+  local width=${#gpu_label}
+  local border
+  border="$(printf 'â”€%.0s' $(seq 1 "$width"))"
+
+  {
+    echo ""
+    echo "â”Œâ”€${border}â”€â”"
+    printf "â”‚ %-${width}s â”‚\n" "$gpu_label"
+    echo "â””â”€${border}â”€â”˜"
     echo ""
     echo "============================================================"
     echo "   ðŸš€ COMFYUI BOOTSTRAP START (Vast) â€” $(date -Is)"
     echo "============================================================"
-    echo " Image Tag:                             ${IMAGE_TAG:-unknown}"
-    echo " Build Git SHA:                         ${BUILD_GIT_SHA:-unknown}"
-    echo ""
-    echo " Repo Root:                             ${REPO_ROOT:-?}"
-    echo " Runtime Directory:                     ${SCRIPT_DIR:-?}"
+    echo " Image Tag:          ${IMAGE_TAG:-unknown}"
+    echo " Build Git SHA:      ${BUILD_GIT_SHA:-unknown}"
     echo ""
-    echo " Python:                                ${pyver}"
-    echo " Torch:                                 ${torchver}"
-    echo " CUDA (Torch):                          ${cudaver}"
+    echo " Repo Root:          ${REPO_ROOT:-?}"
+    echo " Runtime Directory:  ${SCRIPT_DIR:-?}"
     echo ""
-    echo " ComfyUI Path:                          ${COMFY_HOME:-/workspace/ComfyUI}"
+    echo " Python:             ${pyver}"
+    echo " Torch:              ${torchver}"
+    echo " CUDA (Torch):       ${cudaver}"
     echo ""
-    echo " Model Manifest:                        ${MODEL_MANIFEST_URL:-unset}"
-    echo " Custom Node Manifest:                  ${CUSTOM_NODES_MANIFEST_URL:-unset}"
+    echo " ComfyUI Path:       ${COMFY_HOME:-/workspace/ComfyUI}"
+    echo " ComfyUI Version:    ${comfyver}"
     echo ""
-    echo " ENABLE_MODEL_MANIFEST_DOWNLOAD:        ${ENABLE_MODEL_MANIFEST_DOWNLOAD:-true}"
-    echo " ENABLE_CIVITAI_DOWNLOAD:               ${ENABLE_CIVITAI_DOWNLOAD:-true}"
-    echo " ENABLE_SAGE:                           ${ENABLE_SAGE:-true}"
-    echo " INSTALL_CUSTOM_NODES:                  ${INSTALL_CUSTOM_NODES:-true}"
-    echo " LAUNCH_JUPYTER:                        ${LAUNCH_JUPYTER:-false}"
+    echo " Model Manifest:     ${MODEL_MANIFEST_URL:-unset}"
+    echo " Node Manifest:      ${CUSTOM_NODES_MANIFEST_URL:-unset}"
     echo ""
-    show_download_environment_variables
+    echo " ENABLE_MODEL_MANIFEST_DOWNLOAD = ${ENABLE_MODEL_MANIFEST_DOWNLOAD:-1}"
+    echo " ENABLE_CIVITAI_DOWNLOAD        = ${ENABLE_CIVITAI_DOWNLOAD:-1}"
+    echo " ENABLE_SAGE                    = ${ENABLE_SAGE:-1}"
+    echo " INSTALL_EXTRA_CUSTOM_NODES     = ${INSTALL_EXTRA_CUSTOM_NODES:-1}"
+    echo " LAUNCH_JUPYTER                 = ${LAUNCH_JUPYTER:-0}"
     echo ""
-    echo " Log Directory:                         ${COMFY_LOGS:-/workspace/logs}"
+    echo " Log Directory:      ${COMFY_LOGS:-/workspace/logs}"
     echo "============================================================"
     echo ""
   } | tee "$logfile"
@@ -3903,157 +3589,290 @@ setup_ssh() {
   # Start sshd in the background, logging to stdout
   /usr/sbin/sshd -D -e &
   echo "[ssh] sshd started."
-  sleep 5
 }
 
-change_latent_preview_method() {
-  # Honour env toggle, but be robust under set -u
-  local do_change="${change_preview_method:-true}"
-  if [[ "$do_change" != "true" ]]; then
-    echo "[preview] Skipping preview method update (change_preview_method != 'true')."
-    return 0
+clone_or_update_repo() {
+  local url="$1" dir="$2" name
+  name="$(basename "$dir")"
+
+  if [ -d "${dir}/.git" ]; then
+    log "Updating ${name} in ${dir}..."
+    git -C "${dir}" pull --rebase --autostash || \
+      log "Warning: git pull failed for ${name}; using existing checkout."
+  else
+    log "Cloning ${name} from ${url} into ${dir}..."
+    rm -rf "${dir}"
+    git clone --depth 1 "${url}" "${dir}"
   fi
+}
 
-  local comfy_home="${COMFY_HOME:-/workspace/ComfyUI}"
-  local root="${CUSTOM_DIR:-${comfy_home}/custom_nodes}"
-  local js="${root}/ComfyUI-VideoHelperSuite/web/js/VHS.core.js"
+# -------------------------------------------------------------------------------------
+#
+# AI Training environment setup helpers
+#
+#
+# -------------------------------------------------------------------------------------
 
-  echo ""
-  echo "[preview] Attempting to enable VHS.LatentPreview + configure ComfyUI-Managerâ€¦"
-
-  # --- Patch VHS.core.js if present ---
-  if [[ -f "$js" ]]; then
-    echo "[preview] Found VHS JS at: $js"
-    # If sed fails for any reason, don't kill whole bootstrap
-    if ! sed -i "/id: *'VHS.LatentPreview'/,/defaultValue:/s/defaultValue: false/defaultValue: true/" "$js"; then
-      echo "[preview] âš ï¸ Failed to patch $js; leaving as-is." >&2
+tlog() { printf '[start.training] %s\n' "$*"; }
+die() { tlog "ERROR: $*"; exit 1; }
+
+use_venv() {
+  if [ ! -x "${VENV_DIR}/bin/python" ]; then
+    die "Venv not found at ${VENV_DIR}"
+  fi
+  # strip conda from PATH to make venv clearly in charge
+  local PATH_NO_CONDA
+  PATH_NO_CONDA="$(printf '%s\n' "$PATH" | awk -v c="${CONDA_DIR}/bin" -v RS=':' '
+    {
+      if ($0 != c && $0 != "") {
+        out = (out ? out ":" $0 : $0)
+      }
+    }
+    END { print out }
+  ')"
+  export PATH="${VENV_DIR}/bin:${PATH_NO_CONDA}"
+  export PYTHONNOUSERSITE=1
+  tlog "Using venv at ${VENV_DIR} (python: $(command -v python))"
+}
+
+use_conda_env() {
+  [ -d "${CONDA_DIR}" ] || die "Conda dir not found at ${CONDA_DIR}"
+  export PATH="${CONDA_DIR}/bin:${PATH}"
+
+  local CONDA_SH="${CONDA_DIR}/etc/profile.d/conda.sh"
+  if [ -r "${CONDA_SH}" ]; then
+    # shellcheck disable=SC1090
+    . "${CONDA_SH}"
+  else
+    die "conda.sh not found at ${CONDA_SH}"
+  fi
+
+  if ! conda env list | awk '{print $1}' | grep -qx "${CONDA_ENV_NAME}"; then
+    die "Conda env '${CONDA_ENV_NAME}' not found"
+  fi
+
+  tlog "Activating conda env '${CONDA_ENV_NAME}'..."
+  conda activate "${CONDA_ENV_NAME}"
+  tlog "Conda env active. python: $(command -v python)"
+}
+
+# ---------------------------------------------------------
+# Shared helpers (PIPE / misc)
+# ---------------------------------------------------------
+enable_tcmalloc() {
+  if command -v ldconfig >/dev/null 2>&1; then
+    local lib
+    lib="$(ldconfig -p 2>/dev/null | grep -Po 'libtcmalloc.so.\d' | head -n 1 || true)"
+    if [ -n "${lib}" ]; then
+      export LD_PRELOAD="${lib}"
+      tlog "Using tcmalloc: ${lib}"
     else
-      echo "[preview] Patched VHS.LatentPreview defaultValue â†’ true."
+      tlog "libtcmalloc not found; skipping LD_PRELOAD."
     fi
   else
-    echo "[preview] VHS.core.js not found at $js; skipping JS patch."
-  fi
-
-  # --- Configure ComfyUI-Manager config.ini ---
-  local cfg_dir="${comfy_home}/user/default/ComfyUI-Manager"
-  local cfg="${cfg_dir}/config.ini"
-
-  mkdir -p "$cfg_dir"
-
-  if [[ ! -f "$cfg" ]]; then
-    echo "[preview] Creating new ComfyUI-Manager config.ini at: $cfg"
-    cat >"$cfg" <<'INI'
-[default]
-preview_method = auto
-git_exe =
-use_uv = False
-channel_url = https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main
-share_option = all
-bypass_ssl = False
-file_logging = True
-component_policy = workflow
-update_policy = stable-comfyui
-windows_selector_event_loop_policy = False
-model_download_by_agent = False
-downgrade_blacklist =
-security_level = normal
-skip_migration_check = False
-always_lazy_install = False
-network_mode = public
-db_mode = cache
-INI
+    tlog "ldconfig not available; skipping tcmalloc detection."
+  fi
+}
+
+setup_network_volume() {
+  local base
+  if [ -d "/workspace" ]; then
+    base="/workspace"
   else
-    echo "[preview] config.ini already exists. Updating preview_method â†’ autoâ€¦"
-    if ! sed -i 's/^preview_method[[:space:]]*=.*/preview_method = auto/' "$cfg"; then
-      echo "[preview] âš ï¸ Failed to update preview_method in $cfg; leaving as-is." >&2
-    fi
+    base="/workspace"
+    mkdir -p "${base}"
   fi
 
-  echo "[preview] Config file setup complete; default preview method is 'auto'."
-  return 0
+  export NETWORK_VOLUME="${base}/diffusion_pipe_working_folder"
+  mkdir -p "${NETWORK_VOLUME}"
+  echo "cd ${NETWORK_VOLUME}" >> /root/.bashrc
+  tlog "NETWORK_VOLUME=${NETWORK_VOLUME}"
 }
 
-copy_workflows_to_comfyui() {
-  local comfy_home="${COMFY_HOME:-/workspace/ComfyUI}"
-  local source_dir="${comfy_home}/workflows"
-  local dest_root="${COMFY_WORKFLOW_DIR:-${comfy_home}/user/default/workflows}"
-
-  echo ""
-  echo "[workflows] Copying any local workflows..."
-  echo "[workflows] Source:      $source_dir"
-  echo "[workflows] Destination: $dest_root"
+start_jupyter_for_network_volume() {
+  tlog "Starting JupyterLab for NETWORK_VOLUME=${NETWORK_VOLUME}..."
+  jupyter-lab --ip=0.0.0.0 --allow-root --no-browser \
+    --NotebookApp.token='' --NotebookApp.password='' \
+    --ServerApp.allow_origin='*' --ServerApp.allow_credentials=True \
+    --notebook-dir="${NETWORK_VOLUME}" \
+    >/tmp/jupyter.log 2>&1 &
+}
 
-  # If source doesnâ€™t exist or is empty, nothing to do
-  if [[ ! -d "$source_dir" ]]; then
-    echo "[workflows] No source workflows directory found; skipping."
+mirror_tree_if_missing() {
+  local src="$1" dst="$2"
+  if [ -d "${dst}" ]; then
+    tlog "Target ${dst} already exists; leaving as-is."
+    return 0
+  fi
+  if [ ! -d "${src}" ]; then
+    tlog "Warning: source ${src} does not exist; cannot mirror."
     return 0
   fi
+  tlog "Mirroring ${src} -> ${dst}..."
+  mkdir -p "${dst}"
+  rsync -a "${src}/" "${dst}/"
+}
 
-  mkdir -p "$dest_root"
+# ---------------------------------------------------------
+# PIPE mode (diffusion-pipe + Wan LoRA training harness)
+# ---------------------------------------------------------
+run_diffusion_pipe_mode() {
+  section 2 "Initialize Diffusion Pipe workspace"
 
-  shopt -s nullglob
-  local moved_any=0
-  for dir in "$source_dir"/*/; do
-    [[ -d "$dir" ]] || continue
+  enable_tcmalloc
+  setup_network_volume
+  start_jupyter_for_network_volume
 
-    local dir_name dest_dir
-    dir_name="$(basename "$dir")"
-    dest_dir="${dest_root}/${dir_name}"
+  local DP_BASE_SRC="/opt/diffusion-pipe"
+  local HARNESS_SRC="${TRAINING_SRC_DIR}/diffusion-pipe"
+  local DP_WORK_DIR="${NETWORK_VOLUME}/diffusion_pipe"
+  local HARNESS_WORK_DIR="${NETWORK_VOLUME}/runpod-diffusion_pipe"
 
-    if [[ -e "$dest_dir" ]]; then
-      echo "[workflows] Destination already has '$dir_name'. Removing source: $dir"
-      rm -rf -- "$dir"
-    else
-      echo "[workflows] Moving: $dir â†’ $dest_root"
-      mv -- "$dir" "$dest_root/"
-      moved_any=1
+  mirror_tree_if_missing "${DP_BASE_SRC}" "${DP_WORK_DIR}"
+  mirror_tree_if_missing "${HARNESS_SRC}" "${HARNESS_WORK_DIR}"
+
+  if [ -d "${HARNESS_WORK_DIR}" ]; then
+    # Move Captioning + Wan2.2 training helpers to root of working volume
+    [ -d "${HARNESS_WORK_DIR}/Captioning" ] && \
+      mv "${HARNESS_WORK_DIR}/Captioning" "${NETWORK_VOLUME}/"
+    [ -d "${HARNESS_WORK_DIR}/wan2.2_lora_training" ] && \
+      mv "${HARNESS_WORK_DIR}/wan2.2_lora_training" "${NETWORK_VOLUME}/"
+
+    # Optional Qwen helpers
+    if [ "${IS_DEV:-false}" = "true" ] && \
+       [ -d "${HARNESS_WORK_DIR}/qwen_image_musubi_training" ]; then
+      mv "${HARNESS_WORK_DIR}/qwen_image_musubi_training" "${NETWORK_VOLUME}/" 2>/dev/null || true
     fi
-  done
-  shopt -u nullglob
 
-  if (( moved_any == 0 )); then
-    echo "[workflows] No new workflow directories to move."
-  else
-    echo "[workflows] Workflow sync complete."
+    local TOML_DIR="${HARNESS_WORK_DIR}/toml_files"
+    if [ -d "${TOML_DIR}" ]; then
+      tlog "Patching TOMLs under ${TOML_DIR}..."
+      for toml_file in "${TOML_DIR}"/*.toml; do
+        [ -f "${toml_file}" ] || continue
+        cp "${toml_file}" "${toml_file}.backup"
+
+        sed -i "s|diffusers_path = '/models/|diffusers_path = '${NETWORK_VOLUME}/models/|g" "${toml_file}"
+        sed -i "s|ckpt_path = '/Wan/|ckpt_path = '${NETWORK_VOLUME}/models/Wan/|g" "${toml_file}"
+        sed -i "s|checkpoint_path = '/models/|checkpoint_path = '${NETWORK_VOLUME}/models/|g" "${toml_file}"
+        sed -i "s|output_dir = '/data/|output_dir = '${NETWORK_VOLUME}/training_outputs/|g" "${toml_file}"
+        sed -i "s|#transformer_path = '/models/|#transformer_path = '${NETWORK_VOLUME}/models/|g" "${toml_file}"
+      done
+    fi
+
+    if [ -f "${HARNESS_WORK_DIR}/interactive_start_training.sh" ]; then
+      mv "${HARNESS_WORK_DIR}/interactive_start_training.sh" "${NETWORK_VOLUME}/"
+      chmod +x "${NETWORK_VOLUME}/interactive_start_training.sh"
+    fi
+
+    if [ -f "${HARNESS_WORK_DIR}/HowToUse.txt" ]; then
+      mv "${HARNESS_WORK_DIR}/HowToUse.txt" "${NETWORK_VOLUME}/"
+    fi
+
+    if [ -f "${HARNESS_WORK_DIR}/send_lora.sh" ]; then
+      chmod +x "${HARNESS_WORK_DIR}/send_lora.sh"
+      cp "${HARNESS_WORK_DIR}/send_lora.sh" /usr/local/bin/
+    fi
+
+    if [ -d "${DP_WORK_DIR}/examples" ]; then
+      rm -rf "${DP_WORK_DIR}/examples"/*
+      if [ -f "${HARNESS_WORK_DIR}/dataset.toml" ]; then
+        mv "${HARNESS_WORK_DIR}/dataset.toml" "${DP_WORK_DIR}/examples/"
+      fi
+    fi
   fi
 
-  return 0
+  mkdir -p "${NETWORK_VOLUME}/image_dataset_here" \
+           "${NETWORK_VOLUME}/video_dataset_here" \
+           "${NETWORK_VOLUME}/logs"
+
+  if [ -f "${DP_WORK_DIR}/examples/dataset.toml" ]; then
+    sed -i "s|path = '/home/anon/data/images/grayscale'|path = '${NETWORK_VOLUME}/image_dataset_here'|" \
+      "${DP_WORK_DIR}/examples/dataset.toml"
+  fi
+
+  # Triton optional
+  if [ "${download_triton:-false}" = "true" ]; then
+    tlog "Installing Triton..."
+    pip install triton || log "Warning: Triton install failed."
+  fi
+
+  section 3 "Finalize Diffusion Pipe Python stack"
+
+  tlog "Ensuring torch 2.7.1/cu128 for diffusion-pipe..."
+  pip install "torch==2.7.1" "torchvision==0.22.1" "torchaudio==2.7.1" \
+    --index-url https://download.pytorch.org/whl/cu128
+
+  tlog "Upgrading transformers..."
+  pip install -U transformers
+
+  tlog "Ensuring huggingface_hub[cli]..."
+  pip install --upgrade "huggingface_hub[cli]"
+
+  tlog "Upgrading peft (>=0.17.0)..."
+  pip install --upgrade "peft>=0.17.0"
+
+  tlog "Updating diffusers from GitHub..."
+  pip uninstall -y diffusers || true
+  pip install "git+https://github.com/huggingface/diffusers"
+
+  section 4 "Diffusion-pipe ready"
+  tlog "âœ… JupyterLab + Diffusion Pipe workspace ready at ${NETWORK_VOLUME}"
+  sleep infinity
 }
 
-# Pretty section header for logs
-# Usage:
-#   section 1 "bootstrap + env"
-#   section 9 "install extra custom nodes"
-section() {
-  local num="${1:-?}"; shift || true
-  local msg="$*"
+# ---------------------------------------------------------
+# AI-Toolkit mode (TOOLKIT)
+# ---------------------------------------------------------
+run_ai_toolkit_mode() {
+  section 2 "Launch AI-Toolkit UI"
 
-  # Column width: use $COLUMNS if set, otherwise 120; clamp to >= 80
-  local cols="${COLUMNS:-120}"
-  (( cols < 80 )) && cols=80
+  # venv already active
+  cd /opt/ai-toolkit/ui
 
-  # Build a border line like:  ------------------------------------------------------------
-  local border
-  border="$(printf '%*s' "$cols" '' | tr ' ' '-')"
+  local port="${AI_TOOLKIT_PORT:-8675}"
+  tlog "Starting AI-Toolkit UI on port ${port}..."
+  tlog "AI_TOOLKIT_AUTH=${AI_TOOLKIT_AUTH:-<not set>}"
 
-  # Uppercase the message if present
-  if [[ -n "$msg" ]]; then
-    msg="${msg^^}"
-  fi
+  # AI_TOOLKIT_AUTH is passed from Vast secrets
+  AI_TOOLKIT_AUTH="${AI_TOOLKIT_AUTH:-}" \
+  PORT="${port}" \
+    npm run build_and_start
+}
 
-  local ts
-  ts="$(date -Is 2>/dev/null || date)"
+# ---------------------------------------------------------
+# MUSUBI GUI mode (MUSUBI_GUI)
+# ---------------------------------------------------------
+run_musubi_gui_mode() {
+  section 2 "Launch Musubi WAN 2.2 GUI"
 
-  echo
-  echo "$border"
-  if [[ -n "$msg" ]]; then
-    printf "# SECTION %s [@ %s]\n" "$num" "$ts"
-    echo "#"
-    printf "#   %s\n" "$msg"
-  else
-    printf "# SECTION %s [@ %s]\n" "$num" "$ts"
+  # Optional Sage prebuild
+  if declare -f ensure_sage_from_bundle_or_build >/dev/null 2>&1; then
+    tlog "Ensuring SageAttention cache (MUSUBI_GUI)..."
+    ensure_sage_from_bundle_or_build "MUSUBI_GUI"
   fi
-  echo "#"
-  echo "$border"
-  echo
+
+  cd /opt/musubi-tuner_Wan2.2_GUI
+
+  # WANDB_TOKEN can be passed via env; we just expose it to the process
+  tlog "Starting Musubi Wan 2.2 GUI (local desktop app; no HTTP port)."
+  exec python musubi_tuner_gui.py
 }
+
+# ---------------------------------------------------------
+# MUSUBI CLI mode (MUSUBI)
+# ---------------------------------------------------------
+run_musubi_cli_mode() {
+  section 2 "Prepare generic MUSUBI trainer environment"
+
+  if declare -f ensure_sage_from_bundle_or_build >/dev/null 2>&1; then
+    tlog "Ensuring SageAttention cache (MUSUBI)..."
+    ensure_sage_from_bundle_or_build "MUSUBI"
+  fi
+
+  cd /opt/musubi-tuner
+
+  tlog "Generic MUSUBI trainer environment ready."
+  tlog "You can now run musubi-tuner CLI scripts from /opt/musubi-tuner."
+  tlog "Dropping into an interactive shell..."
+  exec bash
+}
\ No newline at end of file

commit 3b030e14ed90a541f5b987458c51f1da37257258
Author: markwelshboy <mark.david.richards@gmail.com>
Date:   Sat Nov 22 21:48:06 2025 -0800

    WIP: 2025-11-22-21:48:06

diff --git a/helpers.sh b/helpers.sh
index 2af665b..41a166a 100644
--- a/helpers.sh
+++ b/helpers.sh
@@ -130,7 +130,7 @@ ensure_dirs(){
 # ------------------------- #
 #  Workflows / Asset import #
 # ------------------------- #
-copy_hearmeman_assets_if_any() {
+copy_hearmeman_files_from_repo_if_any() {
   local repo="${HEARMEMAN_REPO:-}"
   if [[ -z "$repo" ]]; then
     return 0
@@ -143,7 +143,7 @@ copy_hearmeman_assets_if_any() {
   echo "[hearmeman] Temp repo location:  ${tmp}"
 
   if ! git clone "$repo" "$tmp" >/dev/null 2>&1; then
-    echo "[hearmeman] âŒ Failed to clone repo; skipping asset sync." >&2
+    echo "[hearmeman] âŒ Failed to clone repo; skipping repo sync." >&2
     rm -rf "$tmp"
     return 0
   fi
@@ -157,14 +157,17 @@ copy_hearmeman_assets_if_any() {
   fi
 
   if [[ -n "$wf_src" ]]; then
+    echo "[hearmeman] Relocating workflows found in repo."
     local wf_dst="${COMFY_HOME}/workflows"
     echo "[hearmeman] Workflows source:    ${wf_src}"
     echo "[hearmeman] Workflows dest:      ${wf_dst}"
     mkdir -p "$wf_dst"
     cp -rf "${wf_src}/"* "$wf_dst"/ 2>/dev/null || true
   else
-    echo "[hearmeman] No workflows found under src/workflows or workflows." >&2
+    echo "[hearmeman] No workflows found under $repo/src/workflows or $repo/workflows." >&2
   fi
+  
+  echo ""
 
   # ---- Assets ----
   local assets_src=""
@@ -175,17 +178,19 @@ copy_hearmeman_assets_if_any() {
   fi
 
   if [[ -n "$assets_src" ]]; then
+    echo "[hearmeman] Relocating assets found in repo."
     local assets_dst="${COMFY_HOME}/assets"
     echo "[hearmeman] Assets source:       ${assets_src}"
     echo "[hearmeman] Assets dest:         ${assets_dst}"
     mkdir -p "$assets_dst"
     cp -rf "${assets_src}/"* "$assets_dst"/ 2>/dev/null || true
   else
-    echo "[hearmeman] No assets found under src/assets or assets." >&2
+    echo "[hearmeman] No assets found under $repo/src/assets or $repo/assets." >&2
   fi
 
   rm -rf "$tmp"
-  echo "[hearmeman] Asset sync complete."
+  echo ""
+  echo "[hearmeman] Repo sync complete."
 }
 
 # ======================================================================
